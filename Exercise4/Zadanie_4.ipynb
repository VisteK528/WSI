{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Zadanie 4 (7 pkt)\n",
    "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
    "\n",
    "* Implementacja funkcji entropii - **0.5 pkt**\n",
    "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
    "* Implementacja funkcji information gain - **0.5 pkt**\n",
    "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
    "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
   ],
   "metadata": {
    "id": "cpar5LziY_-0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
   ],
   "metadata": {
    "id": "XNc-O3npA-J9",
    "ExecuteTime": {
     "end_time": "2023-11-28T09:46:07.860547779Z",
     "start_time": "2023-11-28T09:46:07.369029342Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 6, 1: 5, 0: 4})\n",
      "dict_items([(1, 5), (2, 6), (0, 4)])\n"
     ]
    },
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Counter(y_test)\n",
    "print(x)\n",
    "print(x.items())\n",
    "y_test.size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:46:08.337931992Z",
     "start_time": "2023-11-28T09:46:08.332374416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def entropy_func(class_count, num_samples):\n",
    "    probability = class_count / num_samples\n",
    "    entropy = - probability * np.log(probability)\n",
    "    return entropy\n",
    "\n",
    "def split(data, classes, split_feature, split_val):\n",
    "    dataset = np.c_[data, classes]\n",
    "    feature_column = dataset[:, split_feature].astype(int)\n",
    "    mask = feature_column > split_val\n",
    "    \n",
    "    child_a = dataset[mask]\n",
    "    child_b = dataset[~mask]\n",
    "    return child_a, child_b\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, group_classes):\n",
    "        self.group_classes = group_classes\n",
    "        self.entropy = self.group_entropy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.group_classes.size\n",
    "\n",
    "    def group_entropy(self):\n",
    "        entropy = 0\n",
    "        class_counts = Counter(self.group_classes)\n",
    "        for group_class, group_class_count in class_counts.items():\n",
    "            entropy += entropy_func(group_class_count, len(self))\n",
    "        return entropy\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
    "        self.split_feature = split_feature\n",
    "        self.split_val = split_val\n",
    "        self.depth = depth\n",
    "        self.child_node_a = child_node_a\n",
    "        self.child_node_b = child_node_b\n",
    "        self.val = val\n",
    "\n",
    "    def predict(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(object):\n",
    "    def __init__(self, max_depth):\n",
    "        self.depth = 0\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_split_entropy(group_a: Group, group_b: Group):\n",
    "        split_entropy = 0\n",
    "        parent_group_count = len(group_a) + len(group_b)\n",
    "        child_groups = [group_a, group_b]\n",
    "        for group in child_groups:\n",
    "            split_entropy += (len(group)/parent_group_count) * group.group_entropy()\n",
    "        return split_entropy\n",
    "\n",
    "    def get_information_gain(self, parent_group: Group, child_group_a: Group, child_group_b: Group):\n",
    "        information_gain = parent_group.group_entropy() - self.get_split_entropy(child_group_a, child_group_b)\n",
    "        return information_gain\n",
    "\n",
    "    def get_best_feature_split(self, feature_values, classes):\n",
    "        parent = Group(classes)\n",
    "        possible_thresholds = np.unique(feature_values)\n",
    "        best_split_val = 0\n",
    "        best_gain = 0\n",
    "        \n",
    "        for threshold in possible_thresholds:\n",
    "            child_a, child_b = split(feature_values, classes, 0, threshold)\n",
    "            child_a = Group(child_a[:, -1])\n",
    "            child_b = Group(child_b[:, -1])\n",
    "            gain = self.get_information_gain(parent, child_a, child_b)\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split_val = threshold\n",
    "        return best_split_val\n",
    "\n",
    "    def get_best_split(self, data, classes):\n",
    "        arguments = data.shape[1]\n",
    "        best_argument = 0\n",
    "        best_gain = 0\n",
    "        best_split = 0\n",
    "        \n",
    "        parent_group = Group(classes)\n",
    "        for argument in range(arguments):\n",
    "            split_val = self.get_best_feature_split(data[:, argument], classes)\n",
    "            child_a = Group(classes[:split])\n",
    "            child_b = Group(classes[split:])\n",
    "            gain = self.get_information_gain(parent_group, child_a, child_b)\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_argument = argument\n",
    "                best_split = split_val\n",
    "                \n",
    "        child_a, child_b = split(data, classes, best_argument, best_split)\n",
    "        return child_a, child_b\n",
    "\n",
    "    def build_tree(self, data, classes, depth=0):\n",
    "        tree_nodes = [Node()]\n",
    "        train_dataset = np.c_(data, classes)\n",
    "        while len(tree_nodes):\n",
    "            tree_node = np.random.choice(tree_nodes)\n",
    "            tree_nodes.remove(tree_node)\n",
    "            if tree_node.depth == 0:\n",
    "                # Oznacz węzeł jako lisc\n",
    "                pass\n",
    "            else:\n",
    "                # Oznacz węzeł jako krawędź\n",
    "                \n",
    "                # Podziel train dataset dla węzła na węzły potomne\n",
    "        pass\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.tree.predict(data)"
   ],
   "metadata": {
    "id": "fBh2tfQ44u5k",
    "ExecuteTime": {
     "end_time": "2023-11-28T09:46:10.660426337Z",
     "start_time": "2023-11-28T09:46:10.655773282Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3454270141.py, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[5], line 67\u001B[0;36m\u001B[0m\n\u001B[0;31m    child_a =\u001B[0m\n\u001B[0m              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dc = DecisionTreeClassifier(3)\n",
    "dc.build_tree(x_train, y_train)\n",
    "for sample, gt in zip(x_test, y_test):\n",
    "    prediction = dc.predict(sample)"
   ],
   "metadata": {
    "id": "U033RY1_YS8x"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
